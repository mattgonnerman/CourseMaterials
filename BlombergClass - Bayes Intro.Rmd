---
title: "A Practical Introduction to Bayesian Modeling"
subtitle: "Wild Turkey Survival in JAGS"
author: "Matthew Gonnerman"
output: html_document
---
```{r, setup, include=FALSE}
#https://mattgonnerman.github.io/CourseMaterials/BlombergClass---Bayes-Intro.html website link

lapply(c("dplyr",
         # "ggplot2", 
         "R2jags",
         "RMark", 
         "gtools"),
       require,
       character.only = T,
       quietly = T) #Whether to report successful loading of packages 
```
### Outline:
[This Lesson...]  
[Bayes Theorem]  
[Describing Our Model System]  
[Building Our Survival Model]  
[From Model to Code]  
[Simulating Our Model System]  
[Running the Model in JAGS: Sampling from the Posterior]  
[Model Diagnostics and Reporting Results]  
[Comparing Bayesian and Frequentist Approaches]  
[Sources and Other Resources]  


### This Lesson...
This exercise is meant as an introductory point for you to learn about the theory behind and application of Bayesian statistics. With that in mind, this will be a surface level exploration of foundational concepts and how to apply these concepts to answer specific research questions. I hope to achieve the following objectives...

<div style="margin-left: 1em;"> 1) To demonstrate how to formulate and code a Bayesian analysis in R from conception through model diagnostics.   
2) To outline common decisions that must be made throughout process and provide relevant background to inform that decision making process. 
3) Compare methods and results from a Bayesian and Frequentist approach to help show when each framework is most appropriate to use.  
 </div>

#### Prepare the R Environment
Before we get started, let's first ensure the R environment has the necessary tools. We will first install, update, and load the packages we will use throughout our code. Just as a reminder, **if you are already working on other R coding projects, updating packages can have cascading effects on code functionality.** If these packages are already installed and have relatively recently been updated, then you are fine to skip this step.  I only add this reminder as many people will go to install/update packages, causing their functioning code to suddenly stop working, leading to an understandable freak out. Luckily, your code is most likely not broken, you will just have to determine which functions were updated and adjust your code to match the new functionality. 

```{r, eval = F}
install.packages("dplyr", "gtools", "R2jags", "RMark")
lapply(c("dplyr", #Data management
         "gtools", #Easy link functions
         "R2jags", "RMark"), #Running models
       require, #Function called to load packages
       character.only = T) #Necessary for lapply functionality
```

In addition to making sure packages are loaded, we will also want to ensure that the programs we are using are correctly installed and communicating with R. We will be using 2 programs for this lesson; [JAGS](https://mcmc-jags.sourceforge.io/) and [MARK](http://www.phidot.org/software/mark/). These programs interact with R2Jags and RMark respectively to run the models we will supply. If not already installed on your computer, you can follow the links provided, navigate to the installation pages and follow the instructions to get these programs installed. Often times, if there are issues with this step, it is because the program was installed in an non-standard location (e.g., "C:\\Program Files").

This is also a good place to share some unsolicited advice which I received too late in my graduate career. Before you actually start coding, stop to think through the organization and workflow of your project, from data collection through assessing model results. Then, identify the major steps to implementing your code and consider breaking up your R script into sections that reflect these steps. Even with extensive comments, having 100's of lines of code within a single script can make reading code and diagnosing issues very difficult. By breaking up and sourcing our scripts like this, we can better create manageable chunks of code that are well organized and easily referenced. For example, here is how I usually break up and source my code, although I often times have even more divisions within each section depending on what I am doing.   
```{r eval = F}
source("1 - DataManagementCode.R")
source("2 - JAGSModelCode.R")
source("3 - RunJAGSCode.R")
source("4 - SummarizeOutput.R")
source("5 - CreateFigures.R")
```

### Bayes Theorem
Before we begin any actual analysis, let's briefly introduce the foundational equation behind Bayesian statistics, called **Bayes Theorem**. Assume there is a study system which is experiencing some relationship which we wish to approximate. Let the unknown effects defining the relationship and any associated uncertainty be represented by $\theta$ and any information we collect to describe the effect be represented as *y*. We wish to ask what are the most likely values for $\theta$ given *y*, otherwise written as $Pr(\theta|y)$. We can approach this question by considering the joint probability of *y* and $\theta$ occurring together, otherwise written as $P(y, \theta)$. This probability can be rewritten using basic rules of conditional probability as shown below...   

$$ Pr(y, \theta) = Pr(\theta|y)Pr(y) = Pr(y|\theta) Pr(\theta)$$  

When we breakdown these probability statements, we find that the probability of both *y* and $\theta$ occurring together is the same as the probability of *y* occurring multiplied by the probability of $\theta$ occurring given that y also occurs (this can also be written with y and $\theta$ reversed). Using basic algebra, we can rearrange our equation to create the below statement, which is Bayes Theorem, and it provides a means to estimate the most appropriate value for $\theta$ given observed values for *y*.  
$$ Pr(\theta|y) = \frac{Pr(y|\theta) Pr(\theta)}{Pr(y)}$$

#### Linking Theory and Information  
Let's briefly breakdown each component within this equation to better understand what each means and what purpose they serve. For more in depth descriptions of Bayes theorem and its components, check out chapter 2 in Statistical Rethinking or chapter 5 in Hobbs and Hooten 2015.

**Posterior Distribution** - $Pr(\theta|y)$  
We will start on the left hand side of the equation with the posterior distribution, which is "the relative plausibility of different values for $\theta$, conditional on the data" (from Statistical Rethinking). This means that we can use the posterior distribution to describe our estimates of unknown parameters based on the data and priors provided to the model. 

A key characteristic of the posterior distribution is that it integrates to 1. This has mathematical implications, but what it means practically is that we are able to me probabilistic statements about $\theta$, which is not possible using other likelihood approaches such as MLE. This also means that unknown values such as $\theta$ can be treated as random variables, which differentiates Bayesian from Frequentist approaches. 

**Likelihood** - $Pr(y|\theta)$  
For those familiar with Maximum Likelihood Estimation, the likelihood of Bayes Theorem functions very similarly, linking unobserved parameters with known information to find the most likely values of $\theta$ given observed values for *y*.   

**Prior Distribution** - $Pr(\theta)$  
The prior distribution represents any information we have about $\theta$ prior to collecting data. This can be in the form of an **informative** or **uninformative** prior distribution. An informative prior implies that we have some prior knowledge about the system we are working with. This could be previous research that has estimated the same or similar parameters as those you are interested and therefore can be used to constrain our estimates. It may also be more vague, such as in the form of expert opinion, which is used to more weakly influence our estimates but still accommodates our expectations about the system. Prior information can be as vague as knowing which type of distribution should be used as a prior for a given parameter. 

In the same way that a prior can represent our knowledge of a parameter, it can also be used to represent that we don't have any prior knowledge of a system and adjust our estimates accordingly. In such cases, we often choose to use some flat prior distribution, otherwise called "uninformative". This nomenclature is a bit confusing because any prior we use implies some information. This could be a uniform distribution across a set of values, a normal distribution with a mean of 0 and a very large standard deviation, or a beta distribution. Each decision relies on some knowledge of the system and what values are reasonable/expected. While integrating our lack of information may seem arbitrary, in practice the use of such priors can have regularizing effects on posterior estimates. This means that even the most basic of priors will have some pull towards the center, which can greatly improve posterior estimates. These regularizing effects are especially useful when working with very limited data.
  
*Aside:* Prior information is often incorrectly attributed as a defining feature of Bayesian statistics. While not often integrated into alternative approaches, use of prior information is not unique to Bayesian statistics but it is required to conduct Bayesian statistics.  

**Joint Distribution** - $Pr(y|\theta)Pr(\theta)$  
The joint distribution is the product of the likelihood and the prior distribution and is proportional to the posterior distribution. This means where the joint distribution is maximized, the posterior distribution will also be maximized.

**Marginal Distribution** - $Pr(y)$  
The marginal distribution defines the probability that the observed data would occur, which is a rather abstract idea. Mathematically, the marginal distribution is equivalent to the integral of the joint distribution and is used to ensure that the posterior distribution integrate to 1. Put simply, dividing by the marginal distribution means that the posterior distribution will be a probabilistic statement. 

Later we will discuss how Bayes theorem is applied to Markov Chain Monte Carlo simulations to sample from the joint distribution and estimate the posterior distribution, but first lets take a break from statistics and introduce a model system that we can use as our example.

### Describing Our Model System
#### Research Question and Hypotheses
For this module, we will be estimating survival rates using repeated observations of an individual. As this lesson is geared towards the Bayesian implementation of already conceived modeling techniques, I won't be going into much detail on the reasoning and derivation of daily survival rate models. For those unfamiliar, I would suggest <a href="https://doi.org/10.1890/0012-9658(2002)083[3476:ATFMAN]2.0.CO;2">Dinsmore et al. 2002</a> or, for a more detailed discussion on the data requirements and formatting, read [the MARK program documentation](http://www.phidot.org/software/mark/docs/book/pdf/chap17.pdf).

Since we all have different backgrounds and study animals, let's posit a hypothetical ecological question that is broadly relevant for management. I spent graduate school studying wild turkey ecology, so let's use survival rates within turkey populations in New England as our study system. While turkeys can subsist on hardwood mast throughout the winter in core areas of the species range, deep snow depths and icy conditions inhibit a turkey's ability to find adequate food sources at their northern range limit. At the same time, low temperatures and high winds make thermoregulation more difficult which increases energetic demands. So knowing that winter is a period of increased stress and mortality for turkeys, we may wish to ask the question **what factors influence daily survival of wild turkeys in winter?** 

To address this question, we will need to propose hypotheses to evaluate. For example, we may expect birds that are generally healthy to be better equipped to deal with harsh conditions and inadequate food. Alternatively, maybe the type of habitat a turkey occupies will influence the availability of natural and anthropogenic food sources. With these two ideas in mind, our hypotheses will be...

<div style="margin-left: 1em;"> 
1) Decreased survival rates will be associated with decreased body condition metrics at capture.
2) Turkeys that primarily occupy suburban landscapes will experience increased survival compared to those that occupy agricultural or forested landscapes.
</div>

#### Data and Unknowns
Clearly stated hypotheses are an important step in producing transparent and unbiased research, but they are also generally useful for providing guidance during model building. By explicitly formulating research questions and potential hypotheses, we are able to identify components that will comprise our model;known and unknown information. Consider that we "view data as the observed realizations of stochastic systems that contain one or several random processes" (Kery 2010). To simplify this statement, our known information is determined by a series of processes that are defined by the unknown information that we wish to estimate. In general, known information is comprised of the data collected to describe some effect or relationships. So in the case of wild turkey winter survival, we could use radio-telemetry to monitor turkeys through the winter. The data we collect will be our known information comprised of the live/dead status of each bird at each visit. It will also include any ancillary information we want to relate to survival, which in our case is individual turkey biometrics (body condition at capture) and landscape information (major land cover class). 

Determining what constitutes relevant unknown information is a more complicated task as it requires familiarity with the statistical methods. This is because, depending on what we think is affecting survival and how we choose to measure it, there are many parameters and sources of variation that we could potentially include into our model. For example, if we were to conduct a mark-recapture study to determine survival, is there some observation process (i.e., variable detection rates) that must be incorporated to account for imperfect detections? Similarly, if we are going to use covariates such as body metrics and habitat to describe variation in survival, we will need to estimate parameters to describe those beta coefficients and the regression intercept. We can also try to account for many of the sources of noise in our system, such as incorporating random effects to describe unmeasured variation among individuals or groups of individuals. As you can tell, the number of parameters we need to estimate increases quickly as we add assumptions and additional data sources. Importantly, though we won't explicitly measure and incorporate every potential variable that influences survival, we can still account for them indirectly in the model through these different error sources, such as random effects. This list could be endless as you can make the model as complicated or simple as you desire, so an important step is differentiating what is relevant to your research question. This will likely be dependent on the question you are asking, your prior knowledge of the system, and the potential monitoring and management options. So to determine what all the unknowns of our proposed system are, let's jump into model construction.

### Building Our Survival Model
We are next going to attempt to construct a daily survival rate model from scratch. There is no correct way to go about model construction, so I will offer up my approach as one option. First, let's identify all known information that we will be inputting into the model. For the purposes of learning, we will ignore the intricacies of coding a model and focus more on first constructing a theoretical model and then code it later. 

First, we have our observed survival information in the form of daily live/dead statuses for each individual visited, which we will abbreviate as *Obs*. We also have the body condition of each bird, *BC*, and whether a turkey was in a suburban, forested (*FOR*), or agriculture (*AG*) dominated landscape. We next need to describe our observed data *Obs* according to the underlying process that determines individual survival outcomes. When we simplify this relationship, it becomes a weighted coin-toss with a binary outcome. Every day a turkey will either survive to the next day or die. For such "coin-toss" dynamics, it is common to use a Bernoulli distribution, which has a single parameter which we will abbreviae to *S*. We can right the relationship between *Obs* and *S* as

$$Obs \sim \text{Bernoulli}(S)$$
which says that the probability of observing a bird alive on a given day follows a bernoulli distribution with a probability of *S*. If we believe that *S* will differ among individuals, we can add notation *i* as a subscript to denote differences in survival among individuals...

$$Obs_i \sim \text{Bernoulli}(S_i)$$

Considering that our hypotheses specify such individual differences in survival associated with body condition and habitat, we will need a means to estimate variation in *S* by these covariates. This can be achieved using linear regression

$$
S_i = \alpha + \boldsymbol{\beta X_i} + \epsilon 
$$

where $\alpha$ is an intercept term, $\boldsymbol{\beta X_i}$ is a series of beta coefficients and associated individual-specific covariate values, and $\epsilon$ represents unspecified variation in *S*. However, because *S* is a probability, it must be constrained between 0 and 1 which is not the case for the above equation. Instead, we can use the logit link function to ensure that *S* remains within realistic boundaries...

$$ 
S_i = \frac{exp(\alpha + \boldsymbol{\beta X_i} + \epsilon)}{1 + exp(\alpha + \boldsymbol{\beta X_i} + \epsilon)}
$$

or written more simply...

$$
\text{logit}(S_i) = \alpha + \boldsymbol{\beta X_i} + \epsilon
$$

So based on the model thus far, our unknown parameters include *S*, $\alpha, \beta, \textrm{ and }\epsilon$. We can estimate all of these with our proposed data, but we will also need to supply some prior values. Considering this is a hypothetical relationship, we will keep things simple and use uninformative priors. Uninformative priors for beta coefficients and error terms such as $\beta \textrm{ and }\epsilon$ can take many forms, but a simple option is a 0 centered normal distribution with a very wide standard deviation 

$$
\beta \sim \text{Normal}(\mu = 0,\sigma = 10000)
$$
Aside: This is a good place to bring up some terms which you will see frequently that are worth differentiating here, parameters versus hyperparameters. A **parameter** defines the system process which we are attempting to estimate and a **hyperparameter** is a parameters of the prior distribution which define other parameters. Think of it as levels, if a parameter defines another parameter via some distribution, then it is a hyperparameter. And a hyperparameter itself can be drawn from a prior with its own hyperparameters. So in our example, $\beta$ would be considered a parameter and $\mu$ and $\sigma$ are hyperparameters.

We will also wish to provide a starting value for $\alpha$, but this will be slightly more complicated. Remember that for a linear regression, $\alpha$ represents the expected value when all covariates are equal to 0. Therefore, $\alpha$ is the probabiltiy of survival when all covariates (*BC*, *FOR*, and *AG*) equal 0 and must be constrained to between 0 and 1. We can do this using a flat beta distribution and the logit link function


$$
\text{logit(}\alpha) = \text{Beta}(1,1)
$$

When we combine all of these statements together, we have a basic Bayesian model for survival

$$
Obs_i \sim \text{Bernoulli}(S_i)\\
\text{logit}(S_i) = \alpha + \boldsymbol{\beta X_i} + \epsilon \\
\text{logit(}\alpha) \sim \text{Beta}(1,1) \\
\beta \sim \text{Normal}(\mu = 0,\sigma = 10000) \\
\epsilon \sim \text{Normal}(\mu = 0,\sigma = 10000)
$$


### From Model to Code
So now that we have a conceptual model, we need to translate the hypothesized s into code that a computer can interpret. Currently, you are most likely to encounter some version of the BUGS language, which is used in a variety of MCMC software including what we will use, JAGS. This language is especially useful for ecologists who may not be familiar with higher level maths, as it allows for the distillation of complex likelihood statements into a list of simple probability distributions. For example, in our model we specify a prior for our regression intercept using an uninformative beta distribution, simply written as ~Beta(1,1), but under the hood JAGS is actually dealing with [more intricate equations](https://en.wikipedia.org/wiki/Beta_distribution#Probability_density_function) when sampling from the posterior. 

There will be many intricacies involved with translating and coding your models, but our proposed survival model provides some excellent beginner modeling examples for common components of a BUGS model. 

#### Nodes
In any model you make, estimated parameters are considered **nodes** within the model, which will often be referenced in any error messages you receive. For every model, nodes must be defined in one of three ways. **Stochastic** nodes are random parameters that are specified by a distribution within the model. Using our above model as an example, *Obs*, $\alpha$, $\beta$, and $\epsilon$ are all stochasitic as they rely on some prior distribution to define them. **Deterministic** nodes are logical functions of other nodes within the model, such as intermediary parameters such as *S*. Finally, **constants** are nodes which have been specified in a separate data file and provided to the model. These are often used as indexing variables which allow for outcomes to be linked with individual/time/location specific covariates (e.g., subscript *i* in the example model will become a constant). Regardless of which type, all nodes must defined by either a prior distribution, other parameters within the model, or by data supplied to the model. Failure to meet this criteria will result in an error message. 

#### Uninformative Normal Prior Distributions
There are many intricacies to selecting and specifying prior distributions which will be context specific and are more complicated than we have the space to discuss. However, the normal distribution, which is specified as dnorm(), is common enough to mention it here. As many of you know, a normal distribution is usually specified by a mean ($\mu$) and standard deviation ($\sigma$). In JAGS however, we use a slightly different specification where instead of standard deviation, dnorm() uses precision ($\tau$) which is just $1/\sigma$. The reasons for this involve the importance of *conjugate priors* in early computing of Bayesian models. You will see this concept discussed if you explore further down Bayesian modeling, but it is not worth going into here. If you wish to read more, try Hobbs and Hooten chapter 5.3 for more on conjugate priors. For now, just remember that whenever you use a normal distribution, you will need to use precision and not standard deviation.

#### Indexing and Dynamic Indexing
There will be occasions where you need to link parameters according to associated factors, such as coefficients within a regression model or when tracking outcomes according to unique individuals. In such cases, you can use indexing just like regular r code. For those unfamiliar, indexing is used in functions such as for loops to iteratively run calculation on a set of values. To apply this to our example, we hypothesized that each individual will experience unique survival rates according to habitat and body condition. To translate this into code for JAGS, we can use the below code to create a for-loop to estimate an individual specific daily survival rate.   

```{r, eval = F}
  for(i in 1:n.ind){
      logit(DSR[i]) <- a.S + b.bc*x.bc[i] + b.ag*x.ag[i] + b.for*x.for[i]
    }
```
Now we have a new problem, we need to make sure each unique observation (i.e., each live/dead status) is linked to the correct survival rate for each turkey. This can be solved with a *dynamic index*, which is vector of constants which themselves are indexes referencing the correct node. 

```{r, eval = F}
for(j in 1:n.obs){
  obs.S[j] ~ dbern(DSR[turkey.id[j]])
}
```
Here, "turkey.id" is a vector with length equal to the number of observations and each value within the vector references a specific turkey (otherwise *i* in the previous for loop). We use index *j* to call a specific turkey ID within the dynamic index, which then calls a specific daily survival rate to use as a probability within the Bernoulli trial.

#### Non-Uniform Data
One of the most important rules in running Bayesian models is that **Missing values (NAs) in supplied data will always cause issues**. Unfortunately, I have never seen a perfect data set and I doubt one exists, which means we have to account for any gaps in visits or missing data. For our example, it is probably unrealistic to assume you will find all birds on all days. Maybe a technician was sick for a day or a specific turkey moved an unexpectedly large distance and could not be found. We can and will adjust our data preparation to account for these gaps in data, but we may also need deal with missing data within our models explicitly. One option (which we will not perform but is worth mentioning) would be to treat missing data as a random variable within the model which needs to be estimated and accounted for. Alternatively, and what we will do, is to incorporate the missed checks into the how we estimate survival. Specifically, we can estimate the probability of surviving any given number of days between successful status check as being equal to the daily survival rate exponentiated by the number of days. In JAGS, we can accomplish this using the pow() function   

```{r, eval = F}
for(j in 1:n.obs){
    period.S[j] <- pow(DSR[turkey.id[j]], interval[j]) # Total probability of survival since previous visit
    obs.S[j] ~ dbern(period.S[j]) # Observation process follows bernoulli distribution
  }
```

If we combine everything we have done above together, we translate our theoretical model into a BUGS model which you can read in full below. Notice that it is wrapped inside of a function, which is purely a functional aspect of the R2JAGS package we are using. 

```{r}
JAGS.turkey.S.model <- function(){
  ###Priors
  base.S ~ dbeta(1,1) 
  a.S <- logit(base.S) 
  b.bc ~ dnorm(0, 1/100) 
  b.ag ~ dnorm(0, 1/100)
  b.for ~ dnorm(0, 1/100)
  
  
  ###LIKELIHOOD
  for(i in 1:n.ind){
      logit(DSR[i]) <- a.S + b.bc*x.bc[i] + b.ag*x.ag[i] + b.for*x.for[i]
    }
  
  for(j in 1:n.obs){
    period.S[j] <- pow(DSR[turkey.id[j]], interval[j]) # Total probability of survival since previous visit
    obs.S[j] ~ dbern(period.S[j]) # Observation process follows bernoulli distribution
  }
}
```

### Simulating Our Model System
#### Simulate individual turkey characteristics and associated weather information
We will use simulated data to test the ability of our modelt to accurately predict survival. This will be useful for demonstrating the concepts we have discussed to this point, but using simulated data also provides a level of control that facilitates model construction. Specifically, by generating data on which our model can be constructed, we will know whether our model outputs are accurate to the real values, which we can specify to be any value we wish. It also allows us to introduce noise into data collection, which will allow us to assess how the model reacts to random variation in data collection. If we were to start with real data, we would have much less capacity to determine whether the values we were producing were accurate.

We are going to start by setting values for the parameters that we will ultimately estimate and compare our model results to. 

```{r}
##Simulated turkey survival
mean.DSR <- 0.99  #base daily survival rate
alpha.s <- logit(mean.DSR) #Use logit link to create an intercept

# Effects on daily survival
beta.body <- .003 #Body Condition
beta.ag <- .5 #Agriculture
beta.for <- -1 #Forested
```

Using the above code, our baseline daily survival rate is set to .99, which means that averaged across all birds, the probability of surviving a given day is 99%. While this may seem high, consider that you will exponentiate this value across multiple days, which adds up fast. Across 10 days, a 99% daily survival rate translate to only a ~90% chance of survival. We will also set the effect sizes for body condition and habitat.You will notice that there are two values for habitat and only one for body condition. That is because we will be treating habitat as a categorical variable (more on that below). 

Now that we have defined how the system will influence turkeys, we need to generate some data to supply to our model. This will be a multiple step process, where we first generate individual turkeys with unique characteristics for body condition and habitat. We will then simulate the real survival history of each bird based on their individual characteristics and the preset system influences we just defined. Finally, we will simulate our observation of that birds survival history. So lets start generating some hypothetical turkeys...

```{r}
#Simulate individual turkey characteristics
n.turkey <- 250  #number of simulated turkeys

turkey.sim <- matrix(NA, nrow = n.turkey, ncol = 5)
for(i in 1:n.turkey){
  #Body Condition Metric, referenced to mean body condition (pre-z-standardized)
  turkey.sim[i,1] <- rnorm(1, mean = 0, sd = 1) 
  
  #Categorical covariate for Landscape, need dummy code it
  h <- sample(1:3, 1) #1 = Suburban, 2 = Agriculture, 3 = Forested
  turkey.sim[i,2] <-  ifelse(h == 2, 1, 0)
  turkey.sim[i,3] <-  ifelse(h == 3, 1, 0)
  
  #Random variation associated with individual turkeys
  turkey.sim[i,4] <- rnorm(1, mean = 0, sd = .1) 
  
  #We can now combine individual betas to define individual survival rates
  turkey.sim[i,5] <- gtools::inv.logit(alpha.s + beta.body*turkey.sim[i,1] + beta.ag*turkey.sim[i,2] +
                                             beta.for*turkey.sim[i,3] + turkey.sim[i,4])
    
}
```
This code starts by defining the total sample size we will be working with, 250 individual turkeys. We then created a matrix where each row corresponds to a turkey and each column to a covariate value. The first column in the matrix defines body condition for each bird, which I assumed to be z-standardized. This makes simulation much easier as I can just use a 0 centered normal distribution, but this can also be useful for comparing effect sizes across multiple covariates which are measured using different units or have different magnitudes. Columns 2 and 3 define whether a bird primarily exists in a suburban, agricultural, or forested landscape. As this is a categorical covariate, we will use "dummy coding" to specify which beta coefficient should apply to each bird. 

You will also notice a fourth column which represents unmeasured variation in survival among individuals. Consider this to be a summary of all of the other aspects of an individuals characteristics and environment which we either don't care about or could not measure. You can adjust these values to be as big or as little as you like, depending on how much additional variation you would normally expect in your data. 

We can then calculate a mean survival rate for each bird using a linear regression as shown in the fifth column.This is the value that will be used in the next step to simulate the "real" survival for each turkey. 

#### Simulate Real Survival History
```{r}
w.days <- 90  #number of days turkeys are monitored for

#We can use a bernoulli trial (weighted coin flip) to simulate survival, assuming a bird was alive, need to make sure that they all start alive at capture, all birds assumed caught Dec 31. 
ind.surv.hist <- matrix(NA, nrow = n.turkey, ncol = w.days+1)
ind.surv.hist[,1] <- 1 #First day is day of capture, set to 1 since known alive

#Combine if/else statement with forloop to simulate
for(i in 1:n.turkey){
  for(t in 2:(w.days+1)){
    if(ind.surv.hist[i, t-1] == 1){ #If the bird was alive at previous visit
      ind.surv.hist[i, t] <- rbinom(1,1, turkey.sim[i,5]) 
    }else{
      break #ends the loop for this bird if it has died
    }
  }
}

#Before we move on, lets change the NAs in the matrix to 0, as once a bird dies it is dead for good.
ind.surv.hist[is.na(ind.surv.hist)] <- 0
```

We can then look at the data to ensure nothing appears out of place. For example, we can check individual histories to ensure they do not violate any rules, such as live statuses occurring after dead statuses. 

```{r}
#Encounter history for first bird
ind.surv.hist[1,]
```

Or we may wish to know how many individuals survived in total, which should give us a rough idea of whether our simulation is correctly specified or if something is not operating correctly.

```{r}
#Lets see how many survived
sum(ind.surv.hist[,w.days + 1])
```

#### Simulate Observed Survival History
Now that we have the true survival history for each bird, we can simulate an observation process as well to better emulate real data. This can be made as complex as you like, but for our purposes we will just assume an average observation rate of 95%, meaning 95% of the time a technician goes to check the status of a turkey they successfully discovered and recorded that status.

```{r}
obs.rate <- .95 #We can't expect to observe each turkey every day, so this just randomizes whether we successfully collect data on a bird on a given day. Let's assume that on average we find a bird every 4 days.

obs.hist.mat <- matrix(NA, nrow = n.turkey, ncol = w.days + 1)
obs.hist.mat[,1] <- 1 #We know the bird was alive at capture
#Now we can simulate our technicians going out and doing the work to collect our data.
for(i in 1:n.turkey){
  for(t in 2:(w.days+1)){
    succ.obs <- rbinom(1,1, obs.rate)
    
    if(succ.obs == 1){
      obs.hist.mat[i, t] <- ind.surv.hist[i, t]
      
      if(ind.surv.hist[i, t] == 0){ #if the bird is dead we would stop checking on it
        break #so we exit loop for this bird when we identify a mortality
      }
    }
  }
}
```

### Running the Model in JAGS: Sampling from the Posterior
#### Markov Chain Monte Carlo Simulations
Once you have your model coded and data collected, you are finally able to evaluate your model. In a Bayesian framework you will have many options for doing this, but by far the most common is **Markov Chain Monte Carlo** simulations. In this method, we simulate samples from the posterior using the joint distribution, which is the product of the likelihood and the prior. How we sample from this distribution will depend on which *sampler* we are using, which is the algorithm defining potential jumps between parameters values. In the case of JAGS we use the *Gibbs sampler* (hence the name, Just Another Gibbs Sampler), but much effort and research is put into developing and refining new samplers to more efficiently simulate MCMC draws of the posterior from a candidate model. Unforunately, JAGS has limited options on which samplers you have available but for our purposes that will be fine. If you begin to explore more complex modelling techniques, you will likely venture into additional MCMC software such as Stan or Nimble, which allow for the use and development of more complex samplers

The steps of a MCMC simulation using a Gibbs sampler follow a general order of operations.

1. An initial starting value along the posterior is chosen, either randomly or by the user.
2. A second random value along the posterior is chosen (the exact decision method will depend on the sampler).
3. Compare the probability of both values being the true value, according to the joint distribution.
4. If the new value has a higher probability of being the true value than the original value, then the sampler moves to the new value.
5. If the original value has a higher probability of being true, then some criteria according to the sampler is used to randomly select one of the two options for the next simulation.
6. Repeat as many times as specified. If more than one parameter is being estimated, each is sampled and assessed individually so that only one parameter value changes for each simulation.

The final result will be a list of values for each parameter from which we can construct a posterior distribution to describe each parameter. 

#### MCMC Decisions - Considerations for Running Time
There are quite a few decisions that you will make when you perform MCMC simulations, such as starting values for the sampler, how many samples to collect, or which sampler to use. These will largely influence model convergence/run time, i.e., how long it takes your model to reach a stable equilibrium around each parameter estimate. At the beginning of MCMC simulations, there is a period where the sampler is searching across the posterior to find its maxima. Once it reaches this point, the sampler should **converge** on a value, i.e., the sampler will remain centered on a value which is otherwise called a **stable equilibrium**. The goal of any MCMC approach is to reach a stable equilibrium for all paramateres as efficiently and quickly as possible.

An issue many people encounter when they first perform MCMC simulations is overagonizing on the initial settings of the MCMC, when in many cases these decisions are subject to the circumstances of the model, data, and researcher. So lets walk through some of these major questions and starting setting you must decide on. These decisions will be largely consistent across software options, although the exact implementation may differ.

**Number of parameters to estimate** - As you would expect, the greater the number of parameters you wish to estiamte, the greater the number of simulations required to produce posterior estiamtes for each.

**Initial Values** - While you can allow the sampler to select a starting location for simulations, you can also specify a realistic starting point as well. For a correctly specified model, this should not affect results, but starting the simulations closer to the posterior maximum can aid in convergence and speed up run times. 

In certain instances, if you let the model choose its own starting values, it can results in impossible relationships, which will cause errors when you try to run the model. For example, if the sampler supplies a negative value to a distribution constrained above 0, then an error will be returned. By specifying a starting value, you can ensure that the sampler begins in a realistic point along the posterior and avoid such errors. 

**Iterations** - This is the total number of iterations (samples) per each parameter chain that the MCMC performs. The longer the chain, the larger the sample size and the more time a parameter estimate has to converge.

**Burn-in Time** - As mentioned above, whether you supply initial values or not, there will be a number of samples collected before the sampler reaches a stable equilibrium. During this time the model has not converged and the values produced should not be considered as part of the posterior distribution. The burn-in period specifies how many of these samples to discard before we begin treating them as part of the posterior. The concern here is that you do not want to use a value so large as to remove viable samples but also it needs to be large enough to allow for convergence. 

**Thinning** - An important aspect of MCMC sampling is that there is no autocorrelation in the samples. Should autocorrelation arise, you can thin the chain to keep every Xth value. There is some debate as to the utility of this method, with some arguing that it increases model run time without necessarily improving inference. If you are looking to efficiently sample the posterior and have little concern for autocorrelation, then a lower thinning value or no thinning at all is advised.

**Sampling Method** - I have already mentioned that different samplers can be used depending on the software and skills at your disposal. Should you venture into more complicated modeling techniques, I would recommend exploring additionally coding options (such as Nimble) to make use of such improved MCMC samplers. Luckily, the language used for coding models across these software is largely very similar, although some care should be taken when first starting to identify important differences. 

**Number of Chains/Parallel Processing** - You also have the option to split your simulations up across multiple chains, which can be considered one set of simulated parameter estimates. Should you have access to a multi-core processor on your computer, your may take advantage of multiple chains and parallel processing to decrease model run time. We will not cover how to do this in this lesson, but for JAGS the process is [relatively simple](https://cran.r-project.org/web/packages/R2jags/R2jags.pdf)  

So with all of that in mind, I have chosen some starting values for the number of iterations, the total burn-in, the thinning rate, and the number of chains. Feel free to adjust these values to see how it affects run time.
```{r}
#Make final MCMC related decisions
ni <- 2000 #Iterations
nb <- 1000 #Burn-In
nt <- 1 #Thinning
nc <- 3 #Chains
```


#### Data Formatting
Before we can send everything to the sampler, we will need to repackage our data in such a way that JAGS and our model can interpret. As far as JAGS is concerned, there are only a few general rules that you will need to worry about. First, whatever form a node is within the model, if it corresponds to supplied data or initial values, then they must be in a similar format. To use our model as an example, observations of live/dead status are specified as a vector, therefore we must reformat the original matrix into an ordered vector. This will require some reference vector (*ID* in the below code) to track which turkey is associated with each observation as well as a vector specifying how many days since the last successful check for each observation (*interval* in the below code.) Alternatively (not shown), you could re-code your model to move through a matrix and supply your encounter history as a matrix. 

```{r}
### Format data to make compatible with model specification
# Remove first column (capture day) from encounter history, will not be used
obs.hist.mat.noDay1 <- obs.hist.mat[,-1]

# JAGS doesn't like NAs, so we need to simplify observation history. 
# An easy way to do this is to convert it to a vector
obs.hist.vec <- as.vector(t(obs.hist.mat.noDay1))
obs.hist <- obs.hist.vec[!is.na(obs.hist.vec)]

# Vector format requires a bit more information to function correctly
# For example, we need a vector to tell the model which bird we are referencing for each visit
ID <- matrix(1:n.turkey, nrow = n.turkey, ncol = w.days)
ID <- as.vector(t(ID))
ID <- ID[!is.na(obs.hist.vec)]      # ID marker

# We also need to know how long since the last visit
visit_ind <- matrix(NA, ncol = ncol(obs.hist.mat), nrow = nrow(obs.hist.mat))
get.last <- function(x) max(which(!is.na(x)))

for(i in 1:n.turkey){
  for(j in 2:(w.days+1)){
    if(is.na(obs.hist.mat[i,j]) == FALSE){
      visit_ind[i,j] <- j - get.last(obs.hist.mat[i,1:(j-1)])
    }
  }
}

interval <- as.vector(t(visit_ind))
interval <- interval[!is.na(interval)]
```

These types of decisions will be largely influenced by requirements of the model you are coding and personal coding preference. Regardless, remember that JAGS works with vectors, matrices, and arrays, meaning that any supplied data should not be a data frame or list. The one exception to this is that you will need to wrap individual data components into a single named list which R can interpret and pass along to JAGS. The names in the list should be exactly the same as the corresponding nodes within the model. This will also be the case for parameter monitors and initial values, as shown below. The *parameter monitors* list tells JAGS which parameters we wish to keep track up. This can be useful for large models with many parameters as you may not wish to track all of them. Practically, your computer will have limited memory for storing these values, so MCMC runs for large models with many iterations will require more storage space than if you only monitored a few important parameters.

```{r}
#Data to supply to the model
data.list <- list(
  #Observation Information
  obs.S = obs.hist, #Vector containing observed survival history
  turkey.id = ID, #Vector describing which turkey an observation was for
  interval = interval,#Time since last visit
  
  n.ind = nrow(turkey.sim), #Number of individual turkeys monitored
  n.obs = length(obs.hist), #Number of total observations
  
  #Covariate Information
  x.bc = turkey.sim[,1], #Vector describing body condition for a given turkey
  x.ag = turkey.sim[,2], #Vector describing whether a turkey primarily uses agriculture landscape
  x.for = turkey.sim[,3] #Vector describing whether a turkey primarily uses forested landscape
)

#Parameter Monitors
parameters.list <-c(
  #Regression Coefficients
  "a.S",
  "b.bc",
  "b.ag",
  "b.for"
)

#Initial Values
inits.list <- function(){
  list(
    base.S = .99 #Set initial value for base survival rate
  )
}
```

#### Running the Model
Once everything is formatted and packaged, we can use the function jags() from R2jags to perform the MCMC simulations. 

```{r, message = F}
#Run the model
Surv_JAGS_output <- R2jags::jags(data = data.list,
                         parameters.to.save = parameters.list,
                         inits = inits.list,
                         model.file = JAGS.turkey.S.model,
                         n.iter = ni,
                         n.burnin = nb,
                         n.thin = nt,
                         n.chains = nc) 
```
#### Common Errors
This is usually the point where, if you made a mistake, you will be met with error messages. As types of errors are numerous and often times difficult to diagnose, below are a few links to lists of common error messages you will see and potential solutions, as identified by experienced modelers. While you will almost always end up searching Google for error messages, it can be useful to familiarize yourself with these common errors so that you know the "general location" of the issue which will be a huge step in diagnosing and resolving issues.

[Heather Gaya - Jags Error Messages](https://github.com/heathergaya/JAGS-NIMBLE-Tutorials/blob/master/JAGS_Errors/JAGS_Error_Messages.pdf)  
[Brian Reich - Understanding error messages in JAGS](https://www4.stat.ncsu.edu/~bjreich/BSMdata/errors.html)  
[Denis Valle - Common problems when using JAGS](https://drvalle1.github.io/99_jags_problems_bad_data.html)  

### Model Diagnostics and Reporting Results
Once JAGS has completed its simulations, you will be returned an object which you can use for model diagnostics and potentially running additional simulations if it failed to converge. The ways in which these results and diagnostics can be performed and reported are numerous, so we will only cover the most common methods used. 

The first thing you will probably want to do once a model is done running is look at the estimates to see if they are reasonable. To extract posterior estimates with confidence intervals, you can either directly print values from the JAGS object...
```{r, results=F}
#Basic Parameter Estimates
print(Surv_JAGS_output)
```

or you can extract the individual chains for more direct interpretation and diagnostics. The only difference being that the below option provides a simpler object format from which to call samples and produce figures and estimates.

```{r}
#Convert BUGS output into MCMC for further information and plotting options
Surv_JAGS_MCMC <- as.mcmc(Surv_JAGS_output)
summary(Surv_JAGS_MCMC)
```

#### Visualizing Results
There are a few common plots that I will introduce here which are regularly reported with results or used for diagnostics

**Posterior Density Plots** - This is simply a density plot showing where the sampler spent its time along a posterior distribution. The more time spent in an area, the greater the probability that value is the true value. Density plots are a simple way to portray a wide range of information relevant to model performance. From these plots we can see where the chains were maximized, what the confidence intervals are, as well as whether the overall shape of the posterior distribution. It is important to remember that, with increased complexity and more factors influencing a given parameter, the more likely it will be that the posterior will not conform to a particular distribution. 

```{r}
lattice::densityplot(Surv_JAGS_MCMC)
```

Aside: This is a relevant point to bring up an important note about interpreting results from logistic regressions, specifically differentiating between **absolute** and **relative** effects. Relative effects refer to the logit link estimate of a beta coefficient and gives the **proportional change in odds** of an outcome.  Absolute effects refer to differences in the final probabilities being estimated and will depend on the entire regression formula. I will point readers to chapter 10 of *Statistical Rethinking* for a more in depth discussion and examples of these two. I bring it up now because we are dealing with effects acting on a event that already has ~99% chance of occurring (surviving a single day). When applying logisitic regression to events that happen with great regularity or extremely rarely, it is often best to evaluate both the relative and absolute differences in the outcome. This is because near these limits (0,1) there is less room for loss/growth and therefore a greater range of values can explain the observed differences in the rate of an outcome.

**Trace Plots** - We can also examine the individual MCMC chains to assess model convergence. For example, if a chain has reached a stable equilibrium, we would not expect the distribution of points to change across the length of a chain. For example, if samples are autocorrelated, you will see a distinct progression of values rather than the random up and down that we would hope to see. This may be difficult to interpret sometimes, but a good general rule is if you can draw a line through the center of the trace plot and never or rarely cross open space, then its possible your model has converged (I have heard many terms for the shape to look for in a trace plot, but my favorite is the "fuzzy caterpillar").
```{r}
traceplot(Surv_JAGS_output, #bugs output object
          mfrow = c(3,2), #Rows and Columns in figure
          ask = F, #Ask before plotting?
          varname = c("a.S", "b.ag", "b.for", "b.bc")) #Which parameters to plot
```

More in depth options for model diagnostics exist, but I will only present a few below for you to explore yourself. 

```{r, eval = F}
require(coda)
gelman.plot(Surv_JAGS_MCMC)

heidel.diag(Surv_JAGS_MCMC)
```

#### Failure to Converge
Should your model fail to converge absent any error messages, you will have several options to choose from. If there is something fundamentally wrong with your analysis, you will need to adjust your model code, data, or initial values and rerun the MCMC simulations. However, it is also possible that you just didn't allow the simulations to run long enough to reach a stable equilibrium yet. In such a case, it may be more about adjusting the number of iterations or burn-in period before dramatically altering your model or data. Instead of rerunning the entire model, you could alternatively use the MCMC object that was initially returned by JAGS and continue simulations using the same model, starting where the previous simulations left off. This can be done using either the **update()** or **autojags()** functions, where update() uses a specified number of iterations and autojags() runs until the model achieves adequate convergence for all paramaters. 

```{r, eval = F}
Surv_JAGS_update <- update(Surv_JAGS_output, n.iter = 1000) #Continues the same simulation, adding iterations 
Surv_JAGS_update <- autojags(Surv_JAGS_output) #Automatically updates model until convergence
```



### Comparing Bayesian and Frequentist Approaches
So you may be thinking, "if its so easy to do this using a Frequentist framework, why in the world did you make me do this long exercise?!."  

#### Similar Results
[Given the same or similar models, you will often times reach the same conclusion whether you are using Bayesian or Frequentist approaches.]

#### Ease of Use
The simple fact is, in most cases the Frequentist approach to analysing a model will be simpler and easier than a Bayesian equivalent. That is, until you begin to introduce complexity into your model or need to perform 

#### Computational Limitations
[Bayesian Simulations Take a lot of time] [Alternative samplers] [Nimble] [Alternative approximation methods]

#### Interpretation of Results
[Intuitive representation of uncertainty] [Hypothetical Replicates vs Probability Statements]

#### Propogation of Uncertainty
[Much easier to account for uncertainty in model parameters used to predict other parameters.]

#### Integrating Models
[Every Posterior can be a prior]

#### Direct Comparison of RMark and R2JAGS results
[Translate simulated data into RMark format]

```{r, error = F, warning = F, message = F, results = "hide"}
rmark.turk.hist <- as.data.frame(obs.hist.mat) %>%
  mutate(FirstFound = 1) %>% 
  rowwise() %>%
  mutate(FirstFound = 1, #All birds found on same day, makes this easy
         LastPresent = max(which(c_across(1:ncol(obs.hist.mat)) == 1)), #What day were they last found alive
         LastChecked = min(which(c_across(1:ncol(obs.hist.mat)) == 0)), #What day were they last checked
         Fate = ifelse(any(c_across(1:ncol(obs.hist.mat)) == 0), 1, 0), #What was the final fate? 0 = Success, 1 = Failure
         Freq = 1) %>% #Frequency of encounter histories with same values and covariates
  mutate(Fate = ifelse(is.infinite(LastChecked), 0, 1),
         LastChecked = ifelse(is.infinite(LastChecked), w.days + 1, LastChecked)) %>%
  dplyr::select(FirstFound, LastPresent, LastChecked, Fate, Freq)

### If you run this code you will get a long warning message which can be ignored

rmark.input <- cbind(rmark.turk.hist, turkey.sim[,1:3]) %>%
  rename(BC = "1", AG = "2", FOR = "3")

WintSurv.process = process.data(rmark.input,
                               model="Nest", #We will run this in a nest survival framework
                               nocc=w.days + 1, #Number of occassions in the EH
                               groups=c("AG", "FOR")) #Identify categorical covariates here
WintSurv.ddl = make.design.data(WintSurv.process)

RMark.model <- mark(WintSurv.process, 
                    WintSurv.ddl, 
                    model.parameters=list(S=list(formula =~ BC + AG + FOR, 
                                                 link="logit")))
```

As a reminder, we initially set as the true values for these parameters as...

```{r, echo = F}
data.frame(MeanDSR = 0.99,  #base daily survival rate
           Intercept = logit(mean.DSR), #Use logit link to create an intercept
           BC = .003, #Body Condition
           AG = .5, #Agriculture
           FOR = -1 #Forested
           )
```
And here are the results we found from MCMC simulations
```{r, echo = F}
summary(Surv_JAGS_MCMC)
```

And finally we can take a look at our results from MARK. 
```{r}
summary(RMark.model)$beta
```
If everything went as it should, then the true parameter values should fall within the confidence intervals for each estimate. There will be some discrepancies between the values because of inherent randomness in our simulation process. Remember, we used random draws from distributions to determine individual outcomes and we also included individual specific differences as represented by $\epsilon$. You should also notice that the estimates from MCMC and MARK are very similar but not exactly the same. Since this is a very basic model, and the only real difference between the models is the inclusion of non-informative priors, we would not expect estimates from a Frequentist or Bayesian approach to differ dramatically. However, as you add complexity, maybe through the inclusion of random effects or integrating additional models to estimate more parameters, then the Frequentist approach will become increasingly less viable an option. Similarly, as data availability decreases, the importance of the regularizing effects of prior distributions also become increasingly important for producing realistic estimates. You can see this yourself by reducing the number of turkeys sampled or the number of days to see how estimates change. 


### Sources and Other Resources

  This module is a synthesis of ideas and code found in...  
<div style="margin-left: 1em;"> 
  [Dan Gibson's Nest Survival Code](https://dan-gibson.weebly.com/nest-survival-jags.html)  
  Hobbs and Hooten - Bayesian Models: A Statistical primer for Ecologists  
  McElreath - Statistical Rethinking: A Bayesian Course with Examples in R and Stan  
  Kery - Introduction to WinBUGS for Ecologists: A Bayesian approach to regression, ANOVA, mixed models and related analyses  
  Kery and Schaub - Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective
</div>
  
I have also found the below resources useful in familiarizing myself with and better understanding Bayesian concepts and applications.  
  <div style="margin-left: 1em;">  
  [Olivier Giminez's Youtube Channel](https://www.youtube.com/c/OlivierGimenez)  
  [Richard McElreath's Youtube Channel](https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/about)  
  [ritvikmath Youtube Channel](https://www.youtube.com/c/ritvikmath)  
  [Fox 2011 - Frequentist vs. Bayesian statistics: resources to help you choose](https://dynamicecology.wordpress.com/2011/10/11/Frequentist-vs-Bayesian-statistics-resources-to-help-you-choose/)  
  [McGill 2013 - Why saying you are a Bayesian is a low information statement](https://dynamicecology.wordpress.com/2013/06/19/why-saying-you-are-a-Bayesian-is-a-low-information-statement/)  
  [rasmusab Youtube Channel](https://www.youtube.com/channel/UCO7kJ__JJ4v4RQU3ZymR3Kw)
  </div>