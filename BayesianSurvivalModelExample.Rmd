---
title: "A Practical Intro to Bayesian Modeling"
subtitle: "Wild Turkey Survival in JAGS/NIMBLE"
author: "Matthew Gonnerman"
output: html_document
---

```{r, setup, include=FALSE}
lapply(c("dplyr",
         "ggplot2", 
         "R2jags",
         "RMark", 
         "gtools"),
       require,
       character.only = T,
       quietly = T) #Whether to report successful loading of packages 
```
### Outline:
[Objectives]  
[Setting Up the R Environment]  
[Describing Our Model System]  
[Simulating our Model System]  
[From Hypothesis to Model]  
[MCMC: Sampling from the Posterior]   
[Translating Model to Code: An Example in JAGS]  
[A Bayesian Alternative: NIMBLE]
[A Frequentist Alternative: RMark]  
[Comparing Bayesian and Frequentist Approaches]  
[Sources and Other Resources]  
  
In graduate school, I wanted to learn how to code and run analyses within a Bayesian framework. I found the entire introductory process to be extremely daunting, partly because it was difficult to find an entry point. While there are a plethora of helpful books which go into great detail on the theory and application of Bayesian statistics, they often required a level of statistical knowledge that was difficult to obtain by simply reading and following along with the code. Now that I have more experience developing and running these models, I consider myself an ecologist with (to steal a phrase from my PhD advisor) "just enough knowledge to be dangerous" 

Instead, it can often be helpful to actually run an analysis first and then learn the concepts second so you have a more practical understanding of how things are applied. It is with this in mind that I have written this module. We are going to work through an survival rate analysis together so that we can learn how to answer research questions under a Bayesian framework. I will expand on relevant Bayesian concepts as they arise, but this should not be considered a comprehensive overview of Bayesian theory and techniques. I will approach our analysis using multiple coding options to provide some idea of the options available to you as you move forward with your work.With all that being said, lets outline exactly what we want to accomplish in this lesson. 

### Objectives

This module is intended as a starting point for learning about the application of Bayesian statistics, specifically aimed at graduate students studying ecology who are unfamiliar or uncomfortable with statistics in general. It should not be considered a comprehensive description but rather as a means of entry that hopefully will give you confidence to further explore these concepts. I have stripped away much of the details to try and distill what is most relevant for ecologists trying to perform an analysis, and not necessarily make you perfect Bayesian statisticians. To that end, I wrote this module with the following objectives in mind...   
  
<div style="margin-left: 1em;"> 1) To demonstrate how to run a bayesian survival analysis in R from conception through model diagnostics.   
2) To outline common decisions that must be made throughout process and provide relevant background to inform that decision making process. 
3) Compare methods and results from a bayesian and frequentist approach to help show when each framework is most appropriate to use.  
4) _To force myself to better understand these concepts and to learn RMarkdown in the process (This one is just for me, but at least now you know why someone would do this in the first place)._ </div>


### Setting Up the R Environment
While I assume that you have familiarity in R, there are some relevant points to touch on as we get started coding. For example, an important initial step before any real code is written is to ensure that all necessary packages and programs are loaded onto your system and communicating properly with R. Obviously, these will differ according to your data, research questions, and preferences.  
  
Let's start with installing and loading the R packages that we will need. I like to work using the dplyr pipeline so we will use that for data management. I usually assume I will be coding my figures in ggplot2 (because apparently I am both a control-freak and a masochist). We will include the gtools and ??? packages which will provide access to some simple functions that streamline our code without having to write out long equations all the time. We will also load the packages that allow us to actually run our models (R2jags, [NIMBLE], RMark)

```{r, eval = F}
install.packages()
lapply(c("dplyr","ggplot2", "gtools", "R2jags", "RMark"),
       require,
       character.only = T,
       quietly = T) #Whether to report successful loading of packages 
```

In addition to making sure packages are loaded, we will also want to ensure that the programs you are using are installed and correctly communicating with R. [Basic flow] [Common Problems]

[Code organization. Sourcing individual ] A final note on getting started. One piece of advice I wish I had gotten sooner was to think through the workflow and split up the code into sections and 
```{r eval = F}
source("1 - DataManagementCode.R")
source("2 - JAGSModelCode.R")
source("3 - RunJAGSCode.R")
source("4 - SummarizeOutput.R")
source("5 - CreateFigures.R")
```


### Describing Our Model System
[Won't focus on decisions specific to the survival analysis and will instead try to highlight decisions and concepts relevant to Bayesian] [For further info on survival rate models, http://www.phidot.org/software/mark/docs/book/pdf/chap17.pdf]

Since we all have different backgrounds and study animals, lets try and answer a straightforward ecological question. I spent my graduate career studying many aspects of wild turkey ecology, so let's use survival rates within turkey populations in New England as our study system. To provide some background, wild turkeys are a wide ranging species of game-bird that is hunted across the US. Because of their status as a game species, there is a considerable interest in maintaining healthy populations to keep hunter satisfaction high. While in core areas of the species range, turkeys can subsist on hardwood mast throughout the winter, at their northern range limit, deep snow depths and icy conditions inhibit a turkeys ability to find adequate food sources. At the same time, low temperatures and high winds make thermoregulation more difficult, increasing energetic demands. A state agency may wish to differentiate between these two causes of mortality as they could lead to different mitigation tacts. If inability to find natural food sources is depressing turkey survival, providing supplemental food sources during the winter would be a viable option for improving survival. If cold temperatures and wind are the issue, then a better management option may be to ensure adequate cold-weather refugia in areas where it is not currently available. So broadly our research question is going to be "How does winter weather impact the survival of wild turkeys in the northeastern United States." We will also want to explicitly state our hypothesis which will aid in model construction.  

<div style="margin-left: 1em;"> 
1) Decreased survival rates will be associated with increased snow depths.
2) Decreased survival rates will be associated with decreased temperatures.
</div>

But maybe we think there are some other relationships that could be driving winter survival, so we will want to test some other hypotheses related to individual turkey characteristics. For example, maybe we decide to collect weight and size information to measure some metric for body condition at capture. This could give us an overall sense of how "healthy" a turkey is when we capture it. Alternatively, maybe the type of habitat a turkey occupies will influence the availability of natural and anthropogenic food sources. While we could collect very detailed to address such a hypothesis, for the purposes of this lesson we will simplify our hypothesis to describe the dominant land cover type found in an individual turkeys home range. So our two additional hypothesis will look like...

<div style="margin-left: 1em;"> 
3) Decreased survival rates will be associated with decreased body condition metrics.
4) Turkeys that primarily occupy forested landscape will experience decreased survival compared to those that occupy agricultural or suburban landscapes.
</div>

[Useful to have all the known parameter values in the same place at the beginning of the code. Makes it easier to change and rerun everything.]
```{r}
##Simulated turkey survival
w.days <- 30  #the length of winter
mean.DSR <- 0.999  #base daily survival rate

alpha.s <- logit(mean.DSR) #Use logit link to create an intercept

# For every 1 sd increase in snow depth compared to the mean, probability of surviving a given day decreases by ...
beta.snow <- -.002 

# For every 1 sd increase in temperature compared to the mean, probability of surviving a given day increases by ...
beta.temp <- -.25 

# For every 1 sd decrease in body condition compared to the mean, probability of surviving a given day decreases by ...
beta.body <- -.003 

# In comparison to suburban landscapes, the probability of surviving a given day changes by ??? for turkeys in a agricultural/forested landscape...
beta.ag <- .1
beta.for <- -.5

```

```{r, echo = F}
winterSR <- mean.DSR^w.days  #base probability of surviving winter
print(paste("Mean Cumulative Winter Survival", winterSR, sep = " = "))

print(paste("Logit-Linked Intercept Value", alpha.s, sep = " = "))
gtools::inv.logit(alpha.s) - gtools::inv.logit(alpha.s+beta.snow)

gtools::inv.logit(alpha.s) - gtools::inv.logit(alpha.s+beta.temp)

gtools::inv.logit(alpha.s) - gtools::inv.logit(alpha.s+beta.body)

gtools::inv.logit(alpha.s) - gtools::inv.logit(alpha.s+beta.for)
```


### Simulating our Model System
So, now we are going to simulate some data that is collected from our simulated population. This may sound a bit silly, but it allows us to [EXPLAIN WHY THIS IS USEFUL] but. So, what kind of data would we collect to monitor survival? We could use radiotelemetry to monitor the live/dead status of a bird. With weekly checks, we could monitor for when a given bird dies, tracking it survival history. 


We are going to survival for hypothetical turkeys in Maine. This has the benefit of letting us know we got the answer correct. So we are going to simulate a number of 3 month survival histories for birds. We are interested in January through March, when winter is the most harsh

Now that our R environment is ready, lets first lets define the relationships. Lets assume temperature and wind are driving short term changes in daily survival rate for this hypthetical population.

Define the relevant variables which will determine the dynamics of our system

[z-standardize, aids in interpretation and simplifies process. Encourage standardizing, can link to or show expample how to do.]


#### Simulate individual turkey characteristics and associated weather information
```{r}
n.turkey <- 200  #number of simulated turkeys

#Simulate individual turkey characteristics
turkey.sim <- matrix(NA, nrow = n.turkey, ncol = 3)
for(i in 1:n.turkey){
  turkey.sim[i,1] <- rnorm(1, mean = 0, sd = 1.5) #Body Condition Metric, referenced to mean body condition (pre-z-standardized)
  #Categorical covariate for Landscape, need dummy code it
  h <- sample(1:3, 1) #1 = Suburban, 2 = Agriculture, 3 = Forested
  turkey.sim[i,2] <-  ifelse(h == 2, 1, 0)
  turkey.sim[i,3] <-  ifelse(h == 3, 1, 0)
}

#Simulate daily weather characteristics
weather.sim <- matrix(NA, nrow = 2, ncol = w.days) #Create an empty matrix to place daily weather information into.
#For loop to populate the matrix
for(j in 1:w.days){
  #Snow Accumulation
  weather.sim[1,j] <- rlnorm(1, meanlog = log(1), sdlog = log(2)) #continuous lognormal distribution
  #Temperature
  weather.sim[2,j] <- rnorm(1, mean = 20, sd = 10) #sample discrete unifrom. option to provide a vector of probabilities to weight samples.
}

#Since we created our weather data to match how it is normally measured, lets next z-standardize the simulated values so they make sense with our beta coefficients.
for(k in 1:2){
  weather.sim[k,] <- scale(weather.sim[k,], center = T, scale = T)
}

# Each turkey will also have unique pressures and differences in behavior which we did not measure but will still cause changes in survival rates, so we need to simulate these influences for each bird at each time step.
random.sim <- matrix(NA, nrow = n.turkey, ncol = w.days)
for(i in 1:n.turkey){
  for(j in 1:w.days){
    random.sim[i,j] <- rnorm(1, mean = 0, sd = 1)
  }
}

#We can now simulate each turkeys individual survival outcomes
#First lets find combine the weather information with the betas we defines to create and unlinked probability
ind.daily.prob <- matrix(NA, nrow = n.turkey, ncol = w.days)
for(i in 1:n.turkey){
  for(t in 1:w.days){
    ind.daily.prob[i,t] <- alpha.s + beta.snow*weather.sim[1,t] + beta.temp*weather.sim[2,t] + beta.body*turkey.sim[i,1] + beta.ag*turkey.sim[i,2] + beta.for*turkey.sim[i,3] + random.sim[i,t]
    
    #We can now use the logit link to tranform into probabilities of surviving a given day
    ind.daily.prob[i,t] <- gtools::inv.logit(ind.daily.prob[i,t])
  }
}

#We can use a bernoulli trial (weighted coin flip) to simulate survival, assuming a bird was alive, need to make sure that they all start alive at capture, all birds assumed caught Dec 31. 

ind.surv.hist <- matrix(NA, nrow = n.turkey, ncol = w.days + 1)
ind.surv.hist[,1] <- 1

#Combine if/else statement with forloop to simulate

for(i in 1:n.turkey){
  for(t in 2:(w.days+1)){
    if(ind.surv.hist[i, t-1] == 1){
      ind.surv.hist[i, t] <- rbinom(1,1, ind.daily.prob[i,(t-1)])
    }else{
      break #ends the loop for this bird if it has died
    }
  }
}

#Before we move on, lets change the NAs in the matrix to 0, as once a bird dies it is dead for good.
ind.surv.hist[is.na(ind.surv.hist)] <- 0

#Encounter history for first bird
ind.surv.hist[1,]

#Lets see how many survived
sum(ind.surv.hist[,w.days + 1], na.rm = T)
 
```
#### Known and Unknown Information

The first step in Bayesian model building is going to involve thinking through your system and identifying all of the known and unknown information that will need to be included in the model. This is why first describing your system and stating the research question and hypothesis is so important. It lays all the information out for you.

Known information will often be some version of the data that you collected. If we wished to study wild turkey survival, a frequently used option will be a radio-telemetry study. 

In the case of our turkey example, the knowns are turkey locations, individual turkey characteristics, and associated land cover at each location. The unknowns are the underlying relationships that define how these two data sets relate to one another. Do turkeys select for a given land cover type consistently more than others? Is there some effect of weather on selection? Maybe a turkey's sex or age affects their decision-making process? What about time of day? While in practice these decisions can be described using increasingly complex modeling techniques, we can simplify a decision to a weighted coin flip describing whether a turkey would select one option over another. For the sake of being on the same page, lets use this system and data as an example throughout the module to describe these coin flips. Specifically, we wish to know how much the presence of snow impacts a turkey's use of forested over other land cover options. 

So now we have our data and the research question we wish to answer, but we still need to determine how we will approach our analysis (ideally this is done in the planning stage of your research). Within the field of ecology, we tend to choose an approaches that fall within one of two paradigms, bayesian or frequentist statistics. There are many similarities and distinctions  in the methods used between the two (which we will summarize at the end of this module), but the core difference lies in how each approaches known and unknown information. 

####Simulate Resulting Survival history
```{r}
obs.rate <- .95 #We can't expect to observe each turkey every day, so this just randomizes whether we successfully collect data on a bird on a given day. Let's assume that on average we find a bird every 4 days.

obs.hist.mat <- matrix(NA, nrow = n.turkey, ncol = w.days + 1)
obs.hist.mat[,1] <- 1 #We know the bird was alive at capture
#Now we can simulate our technicians going out and doing the work to collect our data.
for(i in 1:n.turkey){
  for(t in 2:(w.days+1)){
    succ.obs <- rbinom(1,1, obs.rate)
    
    if(succ.obs == 1){
      obs.hist.mat[i, t] <- ind.surv.hist[i, t]
      
      if(ind.surv.hist[i, t] == 0){ #if the bird is dead we would stop checking on it
        break #so we exit loop for this bird when we identify a mortality
      }
    }
  }
}
```
### From Hypothesis to Model
So lets check in. We have explicitly stated our research question and hypotheses which we wish to test. We have also collected data in the form of daily survival checks and weather information which constitutes our know information. We have also broadly identified our unknown information which we wish to estimate. So the next step is to create our model, which is where we describe the relationship between our known and unknown data. 

From here we are going to take a slight detour to just provide some context for how exactly we find estimates from this model, If you do not care to read about statistical theory (and who could blame you), you can just skip to where we discuss how to write probability statements describing our model ([Writing Our Model Out]).  

#### Bayes Theorem
Very briefly, I want to touch on the statistical theory for why this works, which begins with looking at the **joint probability** of two occurrences happening. To understand this, we will need to use probability theory. We can approach our question by considering the joint probability of our data and $\theta$ occurring together, otherwise written as $P(y, \theta)$. This probability can be rederived using basic rules of conditional probability and written a number of different ways, as shown below...   

$$ P(y, \theta) = Pr(\theta|y)Pr(y) = Pr(y|\theta) Pr(\theta)$$  

When we breakdown these probability statements, we find that the probability of both y and $\theta$ occurring together is the same as the probability of y occurring at all multiplied by the probability of $\theta$ occurring given that y also occurs (this can also be written with y and $\theta$ reversed). Using basic algebra, we can rearrange our equation to create the below statement, also referred to as Bayes Theorem.  


$$ Pr(\theta|y) = \frac{Pr(y|\theta) Pr(\theta)}{Pr(y)}$$
[Broad description]

#### Linking Theory and Information

**Likelihood** - XXX [Confusing nomenclature]  

**Prior Distribution** - XXX  [Defining but not exlusive component of bayesian analysis] [Informative priors]. [What can be a prior?] [Real data] [Expectations] [Benefits]  

[Uninformative priors] In the same way that a prior can represent our knowledge of a parameter, it can also be used to represent that we don't have any prior knowledge of a system and adjust our estimates accordingly, [Benefits of this]  

**Marginal Distribution** - XXX  [Probability of the data] [Can be calculated but doesn't have to be] [Explain basics the segue to...]  

**Joint Distribution** - XXX [Product of the likelihood and the prior] [Proportional to the Posterior. Means that where the joint distribution is maximized, so too will the posterior be maximized. Allows us to ignore the marginal distribution]  

**Posterior Distribution** - [This is the most important quantity in Bayesian inference, goal of the analysis.] [Represents information about unknown parameters based on the data and priors provided.] 

[Every posterior distribution used to be a prior]

#### Writing Our Model Out
Walk through the assumptions of the model and show how to code it. 

ADD A NON TIME VARYING COVARIATE TO DEMONSTRATE HOW THEY ARE CODED DIFFERENTLY

we have an issue, survival is varying according to time-varying covariates, so the model need to reflect that. Coded differently than a non-time varying covariate.

[DISCUSS SUBSCRIPTS/INDEXING]

#### PRIORS
base.S ~ Beta(1,1) # mean survival in restricted to between 0 and 1
a.S ~ log(base.S/(1-base.S)) #use logit link function to transform base.S into an intercept term for the regression
b.snow ~ Normal(mean = 0, sd = 1000)#uninformative prior for beta coefficient of effect of snow
b.temp ~ Normal(mean = -1, sd = 10)#weak informative prior for beta coefficient of effect of temperature
b.bodycondition ~ Normal(mean = -1, sd = 2)#strong but off base informative prior for beta coefficient of effect of average wind speed
b.ag ~ Normal(mean = 0, sd = 1000)#uniformative prior for beta coefficient of effect of agricultural landscape
b.for ~ Normal(mean = 0, sd = 1000)#uniformative prior for beta coefficient of effect of forested landscape
#### LIKELIHOOD
y[i,t] ~ Bernoulli(period.S)
period.S <- S.t1*S.t2*...*S.t?
logit(S.t) <- a.S + b.snow*x.snow[t] + b.ice*x.ice[t] + b.temp*x.temp[t] + b.wind*x.wind[t] 

### MCMC: Sampling from the Posterior
[Bayesian Inference is just counting]

[Simulated draws from the posterior]

#### Markvoc Chain Monte Carlo Simulations

[Potential Sampling Approaches]

### Translating Model to Code: An Example in JAGS
#### MCMC Decisions
There are quite a few options that need to be made when you run a MCMC. These will largely influence model convergence/run time.  [Broadly describe some decions.] The big issue many people run into when making these decisions which can slow them down is they are trying to find some correct answer, when in many cases these decisions are subject to the circumstances of the model, data, and researcher. So lets walk through some of these major questions.  

**Starting Values** - [Where you tell the MCMC to begin sampling for a given parameter value can affect run time] [For a correctly specified model, this shouldn't affect results]  

[In certain instances, if you let the model choose its own starting values, it can results in impossible relationships, which will cause errors when you try to run the model. For example, if you accidentally supply a negative value to a distribution constrained above 0. By specifying a starting value, you allow the model to start in a realistic location]

[Also important to note, these programs hate NA values. So this may be another place where you need to supply starting information so there are no NA values]

**Burn-in Time** -  

**Sampling Method** -  

**Coding Language** - There are many ways that you can code and run an MCMC. We will focus on two coding frameworks that are currently most widely used among ecologists **JAGS** and **NIMBLE**. [BUGS language]. [Similarities]. [Differences]. [When to choose one of the other].  

[Parallel Processing]

#### A JAGS Example
Translate our model into JAGS code

[Write model first, then worry about getting the data formatted correctly - Obviously make sure you aren't writing a model that can't use the data you have]

[Time varying covariate will require some cleverness]

#### Dynamic Indexing
https://mmeredith.net/blog/2021/JAGS_to_NIMBLE.htm

```{r}
JAGS.turkey.S.model <- function(){
  ###Priors
  base.S ~ dbeta(1,1) 
  a.S <- logit(base.S) 
  b.snow ~ dnorm(0, 1/1000)
  b.temp ~ dnorm(-1, 1/10)
  b.bc ~ dnorm(-1, 1/2) 
  b.ag ~ dnorm(0, 1/1000)
  b.for ~ dnorm(0, 1/1000)


  ###LIKELIHOOD
  for(i in 1:n.ind){
    for(t in 1:n.days){
      logit(DSR[i,t]) <- a.S + b.snow*x.snow[t] + b.temp*x.temp[t] + b.bc*x.bc[i] + b.ag*x.ag[i] + b.for*x.for[i]
    }
  }

  for(j in 1:n.obs){
    obs.S[j] ~ dbern(period.S[j]) #Probability of survival since previous check
    period.S[j] <- prod(DSR[turkey.id[j], (day.found[j] - (1:days.since[j]))]) #Aggregrate survival across missed days
  }
}
```
[dnorm using precision instead of sd, common so important to note]

[Indexing to make these work is one of the more difficult things for me.]

[If something is in the equation (= or ~ or <-), then it either needs a prior (b.snow ~ dnorm(0, 1/1000)), be reference to supplied information (obs.S), or be described by estimated or supplied parameters (period.S[i,j] <- prod(DSR[i, missed.days[i,j]])) ]

Reformatting data to get what we want.

```{r}
#[Remove first column because don't care about time before]
obs.hist.mat.noDay1 <- obs.hist.mat[,-1]

#Doesn't like NAs, so we need to remove them from the observation history. An easy way to do this is to convert it to a vector
obs.hist.vec <- as.vector(t(obs.hist.mat.noDay1))
obs.hist <- obs.hist.vec[!is.na(obs.hist.vec)]


#Because of how we specified DSR within the model, we will need a way to reference between our observation and survival rate. We can create two vectors to reference which bird each observation references and which days to reference for a given time period

#First lets find the number of days between each observation
obs.days <- data.frame(which(!is.na(obs.hist.mat), arr.ind = T)) %>% #What day did an observation occur for each turkey
  rename(Turkey = row, ObsDay = col) %>% 
  arrange(Turkey, ObsDay) %>% #Sort by turkey and day
  mutate(DaysSince = ifelse(ObsDay == 1, NA, ObsDay - lag(ObsDay))) #Make sure observations on day 1 don't count from previous bird.

turkey.id.mat <- day.found.mat <- days.since.mat <- matrix(NA, nrow = n.turkey, ncol = w.days+1)

for(i in 1:nrow(obs.days)){
  days.since.mat[obs.days$Turkey[i],obs.days$ObsDay[i]] <- obs.days$DaysSince[i]
}
days.since.vec <- as.vector(t(days.since.mat))
days.since <- days.since.vec[!is.na(days.since.vec)]

#Finally we will need a vector to reference which day a observation occurred on and a separate vector describing which turkey
for(t in 2:(1+w.days)){
  day.found.mat[, t] <- t
}
day.found.mat <- day.found.mat[,-1]
day.found.vec <- as.vector(t(day.found.mat))
day.found <- day.found.vec[!is.na(obs.hist.vec)]

for(i in 1:n.turkey){
  turkey.id.mat[i,] <- i
}
turkey.id.mat <- turkey.id.mat[,-1]
turkey.id.vec <- as.vector(t(turkey.id.mat))
turkey.id <- turkey.id.vec[!is.na(obs.hist.vec)]

```



Packaging everything up. As you can see in our model, we are going to need to reformatting of our data to get it into format our model can udnerstand and use. For example, I made a huge mistake when I initially thought up this model and thought of a time varying covariate (weather). These are much harder to code compared to characteristics of an individual or system that dont change between observations. This is specifically difficult because 
```{r}
#Data to supply to the model
data.list <- list(
  #Observation Information
  obs.S = obs.hist, #Vector containing observed survival history
  days.since = days.since, #Vector describing number of days since previous observation
  day.found = day.found, #Vector describing which day an observation occurred
  turkey.id = turkey.id, #Vector describing which turkey an observation was for
  n.ind = nrow(turkey.sim), #Number of individual turkeys monitored
  n.days = ncol(weather.sim), #Number of days in history
  n.obs = length(obs.hist), #Number of total observations
  
  #Covariate Information
  x.snow = weather.sim[1,], #Vector describing snow depth on a given day
  x.temp = weather.sim[2,], #Vector describing temperature on a given day
  x.bc = turkey.sim[,1], #Vector describing body condition for a given turkey
  x.ag = turkey.sim[,2], #Vector describing whether a turkey primarily uses agriculture landscape
  x.for = turkey.sim[,3] #Vector describing whether a turkey primarily uses forested landscape
)

#Parameter Monitors
parameters.null <-c(
  #Regression Coefficients
  "a.S",
  "b.snow",
  "b.temp",
  "b.bc",
  "b.ag",
  "b.for"
)

#Initial Values
inits.null <- function(){
  list(
    base.S = .998 #Set initial value for base survival rate
  )
}
```

#### Considerations for Run Time
One of the biggest goals of MCMC analysis is to have the model converge as quickly as possible. This helps to save time... There will be many factors affecting run time and many decisions you can make to reduce it. 

**Number of parameters to estimate** - [More nodes means more combinations means more simulations?]  
  
**Data Size** - [Amount of data you have will ]  
  
**Iterations** - [Longer simulations = bigger sample size esentially] 
  
**Burn In Period** - [Model needs time to find a stable equilibrium]    

**Thinning** - [Initially used to help with autocorrelation, but arguments against thinning...wasted data]  

**Number of Chains** -   
  
**Parallel Processing** -  

**Initial Values** - [Starting Closer to posterior maximum can aid in convergence]

[These decisions will be consistent across coding options, although implimentation may differ.]


#### Running the Model  
```{r}
#Make final MCMC related decisions
ni <- 2000 #Number of simulations to run
nb <- 1000 #Length of burnin in period
nt <- 1 #How much thinning should there be
nc <- 3 #Number of chains to run simultaneously

#Run the model
Surv_JAGS_output <- jags(data = data.list,
                        parameters.to.save = parameters.null,
                        inits = inits.null,
                        model.file = JAGS.turkey.S.model,
                        n.iter = ni,
                        n.burnin = nb,
                        n.thin = nt,
                        n.chains = nc) 
```

#### Model Diagnostics and Reporting Results (JAGS)
[Different components of the output object]

[Trace Plots]
```{r}
traceplot(Surv_JAGS_output, #bugs output object
          mfrow = c(3,2), #Rows and Columns in figure
          ask = F, #Ask before plotting?
          varname = c("b.ag", "b.for", "b.bc", "b.snow", "b.temp")) #Which parameters to plot
```

```{r}
#Basic Parameter Estimates
print(Surv_JAGS_output)
```

If we want to run a more in-depth diagnostics of our model, it can be useful to extract the actual MCMC chains. This allows for the running of various diagnostic functions.
```{r}
#Convert BUGS output into MCMC for further information and plotting options
Surv_JAGS_MCMC <- as.mcmc(Surv_JAGS_output)
summary(Surv_JAGS_MCMC)

```

```{r}
require(coda)
gelman.plot(Surv_JAGS_MCMC)

heidel.diag(Surv_JAGS_MCMC)
```

#### Failure to Converge
[Before rewriting your model, it may be worth first adjusting MCMC options] [Interations/BurnIn time] [Starting Values]

https://cran.r-project.org/web/packages/EcoDiet/vignettes/convergence_problems.html

[Using the original MCMC run to continue simulations with the same model]
```{r, eval = F}
Surv_JAGS_update <- update(Surv_JAGS_output, n.iter = 1000) #Continues the same simulation, adding iterations 
Surv_JAGS_update <- autojags(Surv_JAGS_output) #Automatically updates model until convergence
```

#### Evaluating and Presenting Results
[Posterior Distribution]

### A Bayesian Alternative: NIMBLE
[Same thing, but in nimble. Will skim over the theoretical and practical implications of our decisions and just show how to code them. ]
[Why use NIMBLE over JAGS]




[Very quickly show how easily this could be coded using RMark or whatever].  
  
So you may be thinking, "if its so easy to do this using a frequentist framework, why in the world did you make me do this long exercise?!."  

#### Direct Comparisons
**Integrating Models** - XXX  

**Intuitive Description of Uncertainty** - [Hypothetical Replicates vs Probability Statements]  

**Pior information** - XXX  

**Ease of Use** - XXX  

**Computational Limitations** - XXX  


[So now that you know how to do this under a bayesian framework, and you've seen how complicated it is, how would we run the same analysis as a frequentist?]

### A Frequentist Alternative: RMark 
[Phidot link](http://www.phidot.org/software/mark/docs/book/pdf/chap17.pdf)

#### Format the Data
```{r}
EH.rmark <- as.data.frame(obs.hist.mat)[-1,] %>%
  mutate(TurkID = row_number()) %>%
  rowwise() %>%
  mutate(FirstFound = 1, #Conveniently, all of our simulated birds were caught on the same day
         LastPresent = max(which(c_across(1:(w.days)) == 1)), #When was the turkey last observed alive
         LastChecked = max(which(!is.na(c_across(1:(w.days))))), #When was the turkey first observed dead
         Fate = min(c_across(everything()), na.rm = T), #What was the final known fate of a bird through the period of itnerest?
         Freq = 1) %>% #Number of repetitions of a given encounter history, I don't usually aggregate
  mutate(LastChecked = ifelse(Fate == 1, w.days + 1, LastChecked)) %>%
  # mutate(LastChecked = ifelse(FirstDead > LastAlive)) %>%
  dplyr::select(TurkID,
                FirstFound, 
                LastPresent,
                LastChecked,
                Fate,
                Freq)

indcov.rmark <- as.data.frame(turkey.sim) %>%
  rename(BC = V1, Ag = V2, For = V3) %>%
  mutate(TurkID = row_number(), 
         Ag = as.factor(Ag), #RMark will convert automatically, but can preemptively do this to avoid messages
         For = as.factor(For))

data.rmark <- merge(EH.rmark, indcov.rmark)

WintSurv.process = process.data(data.rmark,
                               model="Nest", #We will run this in a nest survival framework
                               nocc=w.days + 1, #Number of occassions in the EH
                               groups=c("Ag", "For")) #Identify categorical covariates here
WintSurv.ddl = make.design.data(WintSurv.process)


## Add Daily Weather data to design data
weather.cov<- data.frame(seq(1,w.days,1))
colnames(weather.cov)<- c("time")
weather.cov$Snow<- weather.sim[1,]
weather.cov$Temp<- weather.sim[2,]

WintSurv.ddl$S=merge_design.covariates(WintSurv.ddl$S,weather.cov,bygroup=FALSE, bytime=TRUE)

```


####Run the model and compare
```{r, error = F, message = F}
require(RMark)
RMark.model <- mark(WintSurv.process, 
                    WintSurv.ddl, 
                    model.parameters=list(S=list(formula =~ BC + Ag + For + Snow + Temp, 
                                                 link="logit")))

summary(RMark.model)

```

### Comparing Bayesian and Frequentist Approaches
**Definition of Unknown Parameters** - XXX  

**Randomness vs Uncertainty** - [Randomness (frequentist) vs uncertainty (Bayesian) - Nothing is actually random, if we had enough information and a powerful enough computer, you could predict almost any natural system in theory]  

**Pior information** - XXX  

**Ease of Use** - XXX  

**Computational Limitations** - XXX  


[Kery and Schaub - Bayesian pop analysis ch 2.4]

[Held and Bove - Likelihood and Bayesian Inference ch 3]

[Hobbs and Hooten - 4.2 Likelihood profiles] 



### Sources and Other Resources

  This module is a synthesis of ideas and code found in...  
<div style="margin-left: 1em;"> 
  [Dan Gibson's Nest Survival Code](https://dan-gibson.weebly.com/nest-survival-jags.html)  
  Hobbs and Hooten - Bayesian Models: A Statistical primer for Ecologists  
  McElreath - Statistical Rethinking: A Bayesian Course with Examples in R and Stan  
  Kery - Introduction to WinBUGS for Ecologists: A Bayesian approach to regression, ANOVA, mixed models and related analyses  
  Kery and Schaub - Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective
</div>
  
I have also found the below resources useful in familiarizing myself with and better understanding bayesian concepts and applications.  
  <div style="margin-left: 1em;">  
  [Olivier Giminez's Youtube Channel](https://www.youtube.com/c/OlivierGimenez)  
  [Richard McElreath's Youtube Channel](https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/about)  
  [ritvikmath Youtube Channel](https://www.youtube.com/c/ritvikmath)  
  [Fox 2011 - Frequentist vs. Bayesian statistics: resources to help you choose](https://dynamicecology.wordpress.com/2011/10/11/frequentist-vs-bayesian-statistics-resources-to-help-you-choose/)  
  [McGill 2013 - Why saying you are a bayesian is a low information statement](https://dynamicecology.wordpress.com/2013/06/19/why-saying-you-are-a-bayesian-is-a-low-information-statement/)  
    
      
  </div>